{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOj8N2GIC9f2IjfGmsxJpKW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/topcat101-cmd/Nids-using-TranferLearning/blob/main/Dataset_deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy5y_LKVenzn"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import itertools\n",
        "import multiprocessing\n",
        "import pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "from collections import OrderedDict\n",
        "%matplotlib inline\n",
        "gt0 = time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B77aqEwFe1--"
      },
      "source": [
        "#!pip install pyspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SQLContext, Row"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaO4GAsfe4u2"
      },
      "source": [
        "conf = SparkConf()\\\n",
        "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\\\n",
        "    .setAppName(\"PySpark NSL-KDD\")\\\n",
        "    .setAll([(\"spark.driver.memory\", \"8g\"), (\"spark.default.parallelism\", f\"{multiprocessing.cpu_count()}\")])\n",
        "\n",
        "# Creating local SparkContext with specified SparkConf and creating SQLContext based on it\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "sc.setLogLevel('INFO')\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKYOvMTufA71"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import udf, split, col\n",
        "import pyspark.sql.functions as sql\n",
        "\n",
        "train20_nsl_kdd_dataset_path = os.path.join(\"NSL_KDD_Dataset\", \"KDDTrain+_20Percent.txt\")\n",
        "train_nsl_kdd_dataset_path = os.path.join(\"NSL_KDD_Dataset\", \"KDDTrain+.txt\")\n",
        "test_nsl_kdd_dataset_path = os.path.join(\"NSL_KDD_Dataset\", \"KDDTest+.txt\")\n",
        "\n",
        "col_names = np.array([\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"labels\"])\n",
        "\n",
        "nominal_inx = [1, 2, 3]\n",
        "binary_inx = [6, 11, 13, 14, 20, 21]\n",
        "numeric_inx = list(set(range(41)).difference(nominal_inx).difference(binary_inx))\n",
        "\n",
        "nominal_cols = col_names[nominal_inx].tolist()\n",
        "binary_cols = col_names[binary_inx].tolist()\n",
        "numeric_cols = col_names[numeric_inx].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptS61LYefNte"
      },
      "source": [
        "def load_dataset(path):\n",
        "    dataset_rdd = sc.textFile(path, 8).map(lambda line: line.split(','))\n",
        "    dataset_df = (dataset_rdd.toDF(col_names.tolist()).select(\n",
        "                    col('duration').cast(DoubleType()),\n",
        "                    col('protocol_type').cast(StringType()),\n",
        "                    col('service').cast(StringType()),\n",
        "                    col('flag').cast(StringType()),\n",
        "                    col('src_bytes').cast(DoubleType()),\n",
        "                    col('dst_bytes').cast(DoubleType()),\n",
        "                    col('land').cast(DoubleType()),\n",
        "                    col('wrong_fragment').cast(DoubleType()),\n",
        "                    col('urgent').cast(DoubleType()),\n",
        "                    col('hot').cast(DoubleType()),\n",
        "                    col('num_failed_logins').cast(DoubleType()),\n",
        "                    col('logged_in').cast(DoubleType()),\n",
        "                    col('num_compromised').cast(DoubleType()),\n",
        "                    col('root_shell').cast(DoubleType()),\n",
        "                    col('su_attempted').cast(DoubleType()),\n",
        "                    col('num_root').cast(DoubleType()),\n",
        "                    col('num_file_creations').cast(DoubleType()),\n",
        "                    col('num_shells').cast(DoubleType()),\n",
        "                    col('num_access_files').cast(DoubleType()),\n",
        "                    col('num_outbound_cmds').cast(DoubleType()),\n",
        "                    col('is_host_login').cast(DoubleType()),\n",
        "                    col('is_guest_login').cast(DoubleType()),\n",
        "                    col('count').cast(DoubleType()),\n",
        "                    col('srv_count').cast(DoubleType()),\n",
        "                    col('serror_rate').cast(DoubleType()),\n",
        "                    col('srv_serror_rate').cast(DoubleType()),\n",
        "                    col('rerror_rate').cast(DoubleType()),\n",
        "                    col('srv_rerror_rate').cast(DoubleType()),\n",
        "                    col('same_srv_rate').cast(DoubleType()),\n",
        "                    col('diff_srv_rate').cast(DoubleType()),\n",
        "                    col('srv_diff_host_rate').cast(DoubleType()),\n",
        "                    col('dst_host_count').cast(DoubleType()),\n",
        "                    col('dst_host_srv_count').cast(DoubleType()),\n",
        "                    col('dst_host_same_srv_rate').cast(DoubleType()),\n",
        "                    col('dst_host_diff_srv_rate').cast(DoubleType()),\n",
        "                    col('dst_host_same_src_port_rate').cast(DoubleType()),\n",
        "                    col('dst_host_srv_diff_host_rate').cast(DoubleType()),\n",
        "                    col('dst_host_serror_rate').cast(DoubleType()),\n",
        "                    col('dst_host_srv_serror_rate').cast(DoubleType()),\n",
        "                    col('dst_host_rerror_rate').cast(DoubleType()),\n",
        "                    col('dst_host_srv_rerror_rate').cast(DoubleType()),\n",
        "                    col('labels').cast(StringType())))\n",
        "\n",
        "    return dataset_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mOm6uPGfTfl"
      },
      "source": [
        "from pyspark.ml import Pipeline, Transformer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark import keyword_only\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
        "\n",
        "# Dictionary that contains mapping of various attacks to the four main categories\n",
        "attack_dict = {\n",
        "    'normal': 'normal',\n",
        "    \n",
        "    'back': 'DoS',\n",
        "    'land': 'DoS',\n",
        "    'neptune': 'DoS',\n",
        "    'pod': 'DoS',\n",
        "    'smurf': 'DoS',\n",
        "    'teardrop': 'DoS',\n",
        "    'mailbomb': 'DoS',\n",
        "    'apache2': 'DoS',\n",
        "    'processtable': 'DoS',\n",
        "    'udpstorm': 'DoS',\n",
        "    \n",
        "    'ipsweep': 'Probe',\n",
        "    'nmap': 'Probe',\n",
        "    'portsweep': 'Probe',\n",
        "    'satan': 'Probe',\n",
        "    'mscan': 'Probe',\n",
        "    'saint': 'Probe',\n",
        "\n",
        "    'ftp_write': 'R2L',\n",
        "    'guess_passwd': 'R2L',\n",
        "    'imap': 'R2L',\n",
        "    'multihop': 'R2L',\n",
        "    'phf': 'R2L',\n",
        "    'spy': 'R2L',\n",
        "    'warezclient': 'R2L',\n",
        "    'warezmaster': 'R2L',\n",
        "    'sendmail': 'R2L',\n",
        "    'named': 'R2L',\n",
        "    'snmpgetattack': 'R2L',\n",
        "    'snmpguess': 'R2L',\n",
        "    'xlock': 'R2L',\n",
        "    'xsnoop': 'R2L',\n",
        "    'worm': 'R2L',\n",
        "    \n",
        "    'buffer_overflow': 'U2R',\n",
        "    'loadmodule': 'U2R',\n",
        "    'perl': 'U2R',\n",
        "    'rootkit': 'U2R',\n",
        "    'httptunnel': 'U2R',\n",
        "    'ps': 'U2R',    \n",
        "    'sqlattack': 'U2R',\n",
        "    'xterm': 'U2R'\n",
        "}\n",
        "\n",
        "attack_mapping_udf = udf(lambda v: attack_dict[v])\n",
        "\n",
        "class Labels2Converter(Transformer):\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self):\n",
        "        super(Labels2Converter, self).__init__()\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        return dataset.withColumn('labels2', sql.regexp_replace(col('labels'), '^(?!normal).*$', 'attack'))\n",
        "     \n",
        "class Labels5Converter(Transformer):\n",
        "    \n",
        "    @keyword_only\n",
        "    def __init__(self):\n",
        "        super(Labels5Converter, self).__init__()\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        return dataset.withColumn('labels5', attack_mapping_udf(col('labels')))\n",
        "    \n",
        "labels2_indexer = StringIndexer(inputCol=\"labels2\", outputCol=\"labels2_index\")\n",
        "labels5_indexer = StringIndexer(inputCol=\"labels5\", outputCol=\"labels5_index\")\n",
        "\n",
        "labels_mapping_pipeline = Pipeline(stages=[Labels2Converter(), Labels5Converter(), labels2_indexer, labels5_indexer])\n",
        "\n",
        "labels2 = ['normal', 'attack']\n",
        "labels5 = ['normal', 'DoS', 'Probe', 'R2L', 'U2R']\n",
        "labels_col = 'labels2_index'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r6ddqeMfUO9"
      },
      "source": [
        "\n",
        "# Loading train data\n",
        "t0 = time()\n",
        "train_df = load_dataset(train_nsl_kdd_dataset_path) ----------'#need to add file path'\n",
        "\n",
        "# Fitting preparation pipeline\n",
        "labels_mapping_model = labels_mapping_pipeline.fit(train_df)\n",
        "\n",
        "# Transforming labels column and adding id column\n",
        "train_df = labels_mapping_model.transform(train_df).withColumn('id', sql.monotonically_increasing_id())\n",
        "\n",
        "train_df = train_df.cache()\n",
        "print(f\"Number of examples in train set: {train_df.count()}\")\n",
        "print(f\"Time: {time() - t0:.2f}s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw5z_VIdfWUF"
      },
      "source": [
        "\n",
        "# Loading test data\n",
        "t0 = time()\n",
        "test_df = load_dataset(test_nsl_kdd_dataset_path) ----------'#need to add file path'\n",
        "\n",
        "# Transforming labels column and adding id column\n",
        "test_df = labels_mapping_model.transform(test_df).withColumn('id', sql.monotonically_increasing_id())\n",
        "\n",
        "test_df = test_df.cache()\n",
        "print(f\"Number of examples in test set: {test_df.count()}\")\n",
        "print(f\"Time: {time() - t0:.2f}s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAbDjiHNOs2c"
      },
      "source": [
        "#import tensorflow as tf # It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks. Tensorflow is a symbolic math library based on dataflow and differentiable programming.\n",
        "#import numpy as np # adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
        "#import pandas as pd #Data manipulating and analysis\n",
        "#import matplotlib.pyplot as plt # Creating static, animated and interactive visualisations within python.\n",
        "#%matplotlib inline\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}