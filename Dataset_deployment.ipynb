{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM25YBH/rHpbhWOhl1T/Okz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/topcat101-cmd/Nids-using-TranferLearning/blob/main/Dataset_deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy5y_LKVenzn"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import itertools\n",
        "import multiprocessing\n",
        "import pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "from collections import OrderedDict\n",
        "%matplotlib inline\n",
        "gt0 = time()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B77aqEwFe1--"
      },
      "source": [
        "!pip install pyspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SQLContext, Row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaO4GAsfe4u2"
      },
      "source": [
        "conf = SparkConf()\\\n",
        "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\\\n",
        "    .setAppName(\"PySpark NSL-KDD\")\\\n",
        "    .setAll([(\"spark.driver.memory\", \"8g\"), (\"spark.default.parallelism\", f\"{multiprocessing.cpu_count()}\")])\n",
        "\n",
        "# Creating local SparkContext with specified SparkConf and creating SQLContext based on it\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "sc.setLogLevel('INFO')\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKYOvMTufA71"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import udf, split, col\n",
        "import pyspark.sql.functions as sql\n",
        "#The \"advantage\" of from xyz import * as opposed to other forms of import is that it imports everything (well, almost...\n",
        "\n",
        "train20_nsl_kdd_dataset_path = os.path.join(\"NSL_KDD_Dataset\", \"KDDTrain+_20Percent.txt\")\n",
        "train_nsl_kdd_dataset_path = os.path.join(\"NSL_KDD_Dataset\", \"KDDTrain+.txt\")\n",
        "test_nsl_kdd_dataset_path = os.path.join(\"NSL_KDD_Dataset\", \"KDDTest+.txt\")\n",
        "\n",
        "col_names = np.array([\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"labels\"])\n",
        "\n",
        "nominal_inx = [1, 2, 3]\n",
        "binary_inx = [6, 11, 13, 14, 20, 21]\n",
        "numeric_inx = list(set(range(41)).difference(nominal_inx).difference(binary_inx))\n",
        "\n",
        "nominal_cols = col_names[nominal_inx].tolist()\n",
        "binary_cols = col_names[binary_inx].tolist()\n",
        "numeric_cols = col_names[numeric_inx].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptS61LYefNte"
      },
      "source": [
        "def load_dataset(path):\n",
        "    dataset_rdd = sc.textFile(path, 8).map(lambda line: line.split(','))\n",
        "    dataset_df = (dataset_rdd.toDF(col_names.tolist()).select(\n",
        "                    col('duration').cast(DoubleType()),\n",
        "                    col('protocol_type').cast(StringType()),\n",
        "                    col('service').cast(StringType()),\n",
        "                    col('flag').cast(StringType()),\n",
        "                    col('src_bytes').cast(DoubleType()),\n",
        "                    col('dst_bytes').cast(DoubleType()),\n",
        "                    col('land').cast(DoubleType()),\n",
        "                    col('wrong_fragment').cast(DoubleType()),\n",
        "                    col('urgent').cast(DoubleType()),\n",
        "                    col('hot').cast(DoubleType()),\n",
        "                    col('num_failed_logins').cast(DoubleType()),\n",
        "                    col('logged_in').cast(DoubleType()),\n",
        "                    col('num_compromised').cast(DoubleType()),\n",
        "                    col('root_shell').cast(DoubleType()),\n",
        "                    col('su_attempted').cast(DoubleType()),\n",
        "                    col('num_root').cast(DoubleType()),\n",
        "                    col('num_file_creations').cast(DoubleType()),\n",
        "                    col('num_shells').cast(DoubleType()),\n",
        "                    col('num_access_files').cast(DoubleType()),\n",
        "                    col('num_outbound_cmds').cast(DoubleType()),\n",
        "                    col('is_host_login').cast(DoubleType()),\n",
        "                    col('is_guest_login').cast(DoubleType()),\n",
        "                    col('count').cast(DoubleType()),\n",
        "                    col('srv_count').cast(DoubleType()),\n",
        "                    col('serror_rate').cast(DoubleType()),\n",
        "                    col('srv_serror_rate').cast(DoubleType()),\n",
        "                    col('rerror_rate').cast(DoubleType()),\n",
        "                    col('srv_rerror_rate').cast(DoubleType()),\n",
        "                    col('same_srv_rate').cast(DoubleType()),\n",
        "                    col('diff_srv_rate').cast(DoubleType()),\n",
        "                    col('srv_diff_host_rate').cast(DoubleType()),\n",
        "                    col('dst_host_count').cast(DoubleType()),\n",
        "                    col('dst_host_srv_count').cast(DoubleType()),\n",
        "                    col('dst_host_same_srv_rate').cast(DoubleType()),\n",
        "                    col('dst_host_diff_srv_rate').cast(DoubleType()),\n",
        "                    col('dst_host_same_src_port_rate').cast(DoubleType()),\n",
        "                    col('dst_host_srv_diff_host_rate').cast(DoubleType()),\n",
        "                    col('dst_host_serror_rate').cast(DoubleType()),\n",
        "                    col('dst_host_srv_serror_rate').cast(DoubleType()),\n",
        "                    col('dst_host_rerror_rate').cast(DoubleType()),\n",
        "                    col('dst_host_srv_rerror_rate').cast(DoubleType()),\n",
        "                    col('labels').cast(StringType())))\n",
        "\n",
        "    return dataset_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mOm6uPGfTfl"
      },
      "source": [
        "from pyspark.ml import Pipeline, Transformer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark import keyword_only\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
        "\n",
        "# Dictionary that contains mapping of various attacks to the four main categories\n",
        "attack_dict = {\n",
        "    'normal': 'normal',\n",
        "    \n",
        "    'back': 'DoS',\n",
        "    'land': 'DoS',\n",
        "    'neptune': 'DoS',\n",
        "    'pod': 'DoS',\n",
        "    'smurf': 'DoS',\n",
        "    'teardrop': 'DoS',\n",
        "    'mailbomb': 'DoS',\n",
        "    'apache2': 'DoS',\n",
        "    'processtable': 'DoS',\n",
        "    'udpstorm': 'DoS',\n",
        "    \n",
        "    'ipsweep': 'Probe',\n",
        "    'nmap': 'Probe',\n",
        "    'portsweep': 'Probe',\n",
        "    'satan': 'Probe',\n",
        "    'mscan': 'Probe',\n",
        "    'saint': 'Probe',\n",
        "\n",
        "    'ftp_write': 'R2L',\n",
        "    'guess_passwd': 'R2L',\n",
        "    'imap': 'R2L',\n",
        "    'multihop': 'R2L',\n",
        "    'phf': 'R2L',\n",
        "    'spy': 'R2L',\n",
        "    'warezclient': 'R2L',\n",
        "    'warezmaster': 'R2L',\n",
        "    'sendmail': 'R2L',\n",
        "    'named': 'R2L',\n",
        "    'snmpgetattack': 'R2L',\n",
        "    'snmpguess': 'R2L',\n",
        "    'xlock': 'R2L',\n",
        "    'xsnoop': 'R2L',\n",
        "    'worm': 'R2L',\n",
        "    \n",
        "    'buffer_overflow': 'U2R',\n",
        "    'loadmodule': 'U2R',\n",
        "    'perl': 'U2R',\n",
        "    'rootkit': 'U2R',\n",
        "    'httptunnel': 'U2R',\n",
        "    'ps': 'U2R',    \n",
        "    'sqlattack': 'U2R',\n",
        "    'xterm': 'U2R'\n",
        "}\n",
        "\n",
        "attack_mapping_udf = udf(lambda v: attack_dict[v])\n",
        "\n",
        "class Labels2Converter(Transformer):\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self):\n",
        "        super(Labels2Converter, self).__init__()\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        return dataset.withColumn('labels2', sql.regexp_replace(col('labels'), '^(?!normal).*$', 'attack'))\n",
        "     \n",
        "class Labels5Converter(Transformer):\n",
        "    \n",
        "    @keyword_only\n",
        "    def __init__(self):\n",
        "        super(Labels5Converter, self).__init__()\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        return dataset.withColumn('labels5', attack_mapping_udf(col('labels')))\n",
        "    \n",
        "labels2_indexer = StringIndexer(inputCol=\"labels2\", outputCol=\"labels2_index\")\n",
        "labels5_indexer = StringIndexer(inputCol=\"labels5\", outputCol=\"labels5_index\")\n",
        "\n",
        "labels_mapping_pipeline = Pipeline(stages=[Labels2Converter(), Labels5Converter(), labels2_indexer, labels5_indexer])\n",
        "\n",
        "labels2 = ['normal', 'attack']\n",
        "labels5 = ['normal', 'DoS', 'Probe', 'R2L', 'U2R']\n",
        "labels_col = 'labels2_index'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stS120T4TDme"
      },
      "source": [
        "The upload of the KDDTest & KDDTrain files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "v9pd0-7n_QCD",
        "outputId": "3fee8bd8-3d02-4e24-9297-635bc852a0cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Default title text\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GMBeFQyMVAj"
      },
      "source": [
        "path = 'drive/My Drive/Dataset/NSL-KDD'"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "nuf-WkoCMWxL",
        "outputId": "848c2f77-7ab6-4ac0-9fe3-86aa43682f75"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "with open(path+'/KDDTest+.arff', 'r') as fp:\n",
        "    file_content = fp.readlines()\n",
        "\n",
        "\n",
        "def parse_row(line, len_row):\n",
        "    line = line.replace('{', '').replace('}', '')\n",
        "\n",
        "    row = np.zeros(len_row)\n",
        "    for data in line.split(','):\n",
        "        index, value = data.split()\n",
        "        row[int(index)] = float(value)\n",
        "\n",
        "    return row\n",
        "\n",
        "\n",
        "columns = []\n",
        "len_attr = len('@attribute')\n",
        "\n",
        "# get the columns\n",
        "for line in file_content:\n",
        "    if line.startswith('@attribute '):\n",
        "        col_name = line[len_attr:].split()[0]\n",
        "        columns.append(col_name)\n",
        "\n",
        "rows = []\n",
        "len_row = len(columns)\n",
        "# get the rows\n",
        "for line in file_content:\n",
        "    if line.startswith('{'):\n",
        "        rows.append(parse_row(line, len_row))\n",
        "\n",
        "df = pd.DataFrame(data=rows, columns=columns)\n",
        "\n",
        "df.head()\n",
        "#https://stackoverflow.com/questions/59271661/cannot-load-arff-dataset-with-scipy-arff-loadarff"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>'duration'</th>\n",
              "      <th>'protocol_type'</th>\n",
              "      <th>'service'</th>\n",
              "      <th>'flag'</th>\n",
              "      <th>'src_bytes'</th>\n",
              "      <th>'dst_bytes'</th>\n",
              "      <th>'land'</th>\n",
              "      <th>'wrong_fragment'</th>\n",
              "      <th>'urgent'</th>\n",
              "      <th>'hot'</th>\n",
              "      <th>'num_failed_logins'</th>\n",
              "      <th>'logged_in'</th>\n",
              "      <th>'num_compromised'</th>\n",
              "      <th>'root_shell'</th>\n",
              "      <th>'su_attempted'</th>\n",
              "      <th>'num_root'</th>\n",
              "      <th>'num_file_creations'</th>\n",
              "      <th>'num_shells'</th>\n",
              "      <th>'num_access_files'</th>\n",
              "      <th>'num_outbound_cmds'</th>\n",
              "      <th>'is_host_login'</th>\n",
              "      <th>'is_guest_login'</th>\n",
              "      <th>'count'</th>\n",
              "      <th>'srv_count'</th>\n",
              "      <th>'serror_rate'</th>\n",
              "      <th>'srv_serror_rate'</th>\n",
              "      <th>'rerror_rate'</th>\n",
              "      <th>'srv_rerror_rate'</th>\n",
              "      <th>'same_srv_rate'</th>\n",
              "      <th>'diff_srv_rate'</th>\n",
              "      <th>'srv_diff_host_rate'</th>\n",
              "      <th>'dst_host_count'</th>\n",
              "      <th>'dst_host_srv_count'</th>\n",
              "      <th>'dst_host_same_srv_rate'</th>\n",
              "      <th>'dst_host_diff_srv_rate'</th>\n",
              "      <th>'dst_host_same_src_port_rate'</th>\n",
              "      <th>'dst_host_srv_diff_host_rate'</th>\n",
              "      <th>'dst_host_serror_rate'</th>\n",
              "      <th>'dst_host_srv_serror_rate'</th>\n",
              "      <th>'dst_host_rerror_rate'</th>\n",
              "      <th>'dst_host_srv_rerror_rate'</th>\n",
              "      <th>'class'</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'class']\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCoqaFuATViW"
      },
      "source": [
        "The End of the upload of the KDDTest & KDDTrain files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r6ddqeMfUO9"
      },
      "source": [
        "\n",
        "# Loading train data\n",
        "def loading_train_data():\n",
        "  t0 = time()\n",
        "  train_df = load_dataset(train_nsl_kdd_dataset_path) #----------'#need to add file path'\n",
        "\n",
        "# Fitting preparation pipeline\n",
        "  labels_mapping_model = labels_mapping_pipeline.fit(train_df)\n",
        "\n",
        "# Transforming labels column and adding id column\n",
        "  train_df = labels_mapping_model.transform(train_df).withColumn('id', sql.monotonically_increasing_id())\n",
        "\n",
        "  train_df = train_df.cache()\n",
        "  print(f\"Number of examples in train set: {train_df.count()}\")\n",
        "  print(f\"Time: {time() - t0:.2f}s\")\n",
        "  return loading_train_data\n",
        "#print (loading_train_data()) #Testing purposes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw5z_VIdfWUF"
      },
      "source": [
        "\n",
        "# Loading test data\n",
        "def loading_test_data():\n",
        "  t0 = time()\n",
        "  test_df = load_dataset(test_nsl_kdd_dataset_path) #----------'#need to add file path'\n",
        "\n",
        "# Transforming labels column and adding id column\n",
        "  test_df = labels_mapping_model.transform(test_df).withColumn('id', sql.monotonically_increasing_id())\n",
        "\n",
        "  test_df = test_df.cache()\n",
        "  print(f\"Number of examples in test set: {test_df.count()}\")\n",
        "  print(f\"Time: {time() - t0:.2f}s\")\n",
        "  return loading_test_data\n",
        "#print (loading_test_data()) #Testing purposes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rvRe6o5-3Sy"
      },
      "source": [
        "import tensorflow as tf # It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks. Tensorflow is a symbolic math library based on dataflow and differentiable programming.\n",
        "import numpy as np # adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
        "import pandas as pd #Data manipulating and analysis\n",
        "import matplotlib.pyplot as plt # Creating static, animated and interactive visualisations within python.\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}