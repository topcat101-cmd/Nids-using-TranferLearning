{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOHYkA+ZCIN7mjC/6l963yN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/topcat101-cmd/Nids-using-TranferLearning/blob/main/Dataset_deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy5y_LKVenzn"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import itertools\n",
        "import multiprocessing\n",
        "import pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from time import time\n",
        "from collections import OrderedDict\n",
        "%matplotlib inline\n",
        "gt0 = time()\n",
        "\n",
        "#import tensorflow as tf # It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks. Tensorflow is a symbolic math library based on dataflow and differentiable programming.\n",
        "#import numpy as np # adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
        "#import pandas as pd #Data manipulating and analysis\n",
        "#import matplotlib.pyplot as plt # Creating static, animated and interactive visualisations within python.\n",
        "#%matplotlib inline"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B77aqEwFe1--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ea90dc-d913-404c-df86-add26175cd45"
      },
      "source": [
        "!pip install pyspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SQLContext, Row"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 63kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 15.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=069f92b0eb586b1a0eeb80f12567948b00eae3187a3b7cb98c13fbfcd78d5cf6\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaO4GAsfe4u2"
      },
      "source": [
        "conf = SparkConf()\\\n",
        "    .setMaster(f\"local[{multiprocessing.cpu_count()}]\")\\\n",
        "    .setAppName(\"PySpark NSL-KDD\")\\\n",
        "    .setAll([(\"spark.driver.memory\", \"8g\"), (\"spark.default.parallelism\", f\"{multiprocessing.cpu_count()}\")])\n",
        "\n",
        "# Creating local SparkContext with specified SparkConf and creating SQLContext based on it\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "sc.setLogLevel('INFO')\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKYOvMTufA71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "268b6448-3fb5-4952-86f3-8d7eda855bf7"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import udf, split, col\n",
        "import pyspark.sql.functions as sql\n",
        "#The \"advantage\" of from xyz import * as opposed to other forms of import is that it imports everything (well, almost...\n",
        "\n",
        "train20_nsl_kdd_dataset_path = os.path.join(\"NSL_KDD_Dataset\", \"KDDTrain+_20Percent.txt\")\n",
        "train_nsl_kdd_dataset_path = os.path.join(\"NSL_KDD_Dataset\", \"KDDTrain+.txt\")\n",
        "test_nsl_kdd_dataset_path = os.path.join(\"NSL_KDD_Dataset\", \"KDDTest+.txt\")\n",
        "\n",
        "col_names = np.array([\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"labels\"])\n",
        "\n",
        "nominal_inx = [1, 2, 3]\n",
        "binary_inx = [6, 11, 13, 14, 20, 21]\n",
        "numeric_inx = list(set(range(41)).difference(nominal_inx).difference(binary_inx))\n",
        "\n",
        "nominal_cols = col_names[nominal_inx].tolist()\n",
        "binary_cols = col_names[binary_inx].tolist()\n",
        "numeric_cols = col_names[numeric_inx].tolist()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-69ab964cf7a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#The \"advantage\" of from xyz import * as opposed to other forms of import is that it imports everything (well, almost...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptS61LYefNte"
      },
      "source": [
        "def load_dataset(path):\n",
        "    dataset_rdd = sc.textFile(path, 8).map(lambda line: line.split(','))\n",
        "    dataset_df = (dataset_rdd.toDF(col_names.tolist()).select(\n",
        "                    col('duration').cast(DoubleType()),\n",
        "                    col('protocol_type').cast(StringType()),\n",
        "                    col('service').cast(StringType()),\n",
        "                    col('flag').cast(StringType()),\n",
        "                    col('src_bytes').cast(DoubleType()),\n",
        "                    col('dst_bytes').cast(DoubleType()),\n",
        "                    col('land').cast(DoubleType()),\n",
        "                    col('wrong_fragment').cast(DoubleType()),\n",
        "                    col('urgent').cast(DoubleType()),\n",
        "                    col('hot').cast(DoubleType()),\n",
        "                    col('num_failed_logins').cast(DoubleType()),\n",
        "                    col('logged_in').cast(DoubleType()),\n",
        "                    col('num_compromised').cast(DoubleType()),\n",
        "                    col('root_shell').cast(DoubleType()),\n",
        "                    col('su_attempted').cast(DoubleType()),\n",
        "                    col('num_root').cast(DoubleType()),\n",
        "                    col('num_file_creations').cast(DoubleType()),\n",
        "                    col('num_shells').cast(DoubleType()),\n",
        "                    col('num_access_files').cast(DoubleType()),\n",
        "                    col('num_outbound_cmds').cast(DoubleType()),\n",
        "                    col('is_host_login').cast(DoubleType()),\n",
        "                    col('is_guest_login').cast(DoubleType()),\n",
        "                    col('count').cast(DoubleType()),\n",
        "                    col('srv_count').cast(DoubleType()),\n",
        "                    col('serror_rate').cast(DoubleType()),\n",
        "                    col('srv_serror_rate').cast(DoubleType()),\n",
        "                    col('rerror_rate').cast(DoubleType()),\n",
        "                    col('srv_rerror_rate').cast(DoubleType()),\n",
        "                    col('same_srv_rate').cast(DoubleType()),\n",
        "                    col('diff_srv_rate').cast(DoubleType()),\n",
        "                    col('srv_diff_host_rate').cast(DoubleType()),\n",
        "                    col('dst_host_count').cast(DoubleType()),\n",
        "                    col('dst_host_srv_count').cast(DoubleType()),\n",
        "                    col('dst_host_same_srv_rate').cast(DoubleType()),\n",
        "                    col('dst_host_diff_srv_rate').cast(DoubleType()),\n",
        "                    col('dst_host_same_src_port_rate').cast(DoubleType()),\n",
        "                    col('dst_host_srv_diff_host_rate').cast(DoubleType()),\n",
        "                    col('dst_host_serror_rate').cast(DoubleType()),\n",
        "                    col('dst_host_srv_serror_rate').cast(DoubleType()),\n",
        "                    col('dst_host_rerror_rate').cast(DoubleType()),\n",
        "                    col('dst_host_srv_rerror_rate').cast(DoubleType()),\n",
        "                    col('labels').cast(StringType())))\n",
        "\n",
        "    return dataset_df"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mOm6uPGfTfl"
      },
      "source": [
        "from pyspark.ml import Pipeline, Transformer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark import keyword_only\n",
        "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
        "\n",
        "# Dictionary that contains mapping of various attacks to the four main categories\n",
        "attack_dict = {\n",
        "    'normal': 'normal',\n",
        "    \n",
        "    'back': 'DoS',\n",
        "    'land': 'DoS',\n",
        "    'neptune': 'DoS',\n",
        "    'pod': 'DoS',\n",
        "    'smurf': 'DoS',\n",
        "    'teardrop': 'DoS',\n",
        "    'mailbomb': 'DoS',\n",
        "    'apache2': 'DoS',\n",
        "    'processtable': 'DoS',\n",
        "    'udpstorm': 'DoS',\n",
        "    \n",
        "    'ipsweep': 'Probe',\n",
        "    'nmap': 'Probe',\n",
        "    'portsweep': 'Probe',\n",
        "    'satan': 'Probe',\n",
        "    'mscan': 'Probe',\n",
        "    'saint': 'Probe',\n",
        "\n",
        "    'ftp_write': 'R2L',\n",
        "    'guess_passwd': 'R2L',\n",
        "    'imap': 'R2L',\n",
        "    'multihop': 'R2L',\n",
        "    'phf': 'R2L',\n",
        "    'spy': 'R2L',\n",
        "    'warezclient': 'R2L',\n",
        "    'warezmaster': 'R2L',\n",
        "    'sendmail': 'R2L',\n",
        "    'named': 'R2L',\n",
        "    'snmpgetattack': 'R2L',\n",
        "    'snmpguess': 'R2L',\n",
        "    'xlock': 'R2L',\n",
        "    'xsnoop': 'R2L',\n",
        "    'worm': 'R2L',\n",
        "    \n",
        "    'buffer_overflow': 'U2R',\n",
        "    'loadmodule': 'U2R',\n",
        "    'perl': 'U2R',\n",
        "    'rootkit': 'U2R',\n",
        "    'httptunnel': 'U2R',\n",
        "    'ps': 'U2R',    \n",
        "    'sqlattack': 'U2R',\n",
        "    'xterm': 'U2R'\n",
        "}\n",
        "\n",
        "attack_mapping_udf = udf(lambda v: attack_dict[v])\n",
        "\n",
        "class Labels2Converter(Transformer):\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self):\n",
        "        super(Labels2Converter, self).__init__()\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        return dataset.withColumn('labels2', sql.regexp_replace(col('labels'), '^(?!normal).*$', 'attack'))\n",
        "     \n",
        "class Labels5Converter(Transformer):\n",
        "    \n",
        "    @keyword_only\n",
        "    def __init__(self):\n",
        "        super(Labels5Converter, self).__init__()\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        return dataset.withColumn('labels5', attack_mapping_udf(col('labels')))\n",
        "    \n",
        "labels2_indexer = StringIndexer(inputCol=\"labels2\", outputCol=\"labels2_index\")\n",
        "labels5_indexer = StringIndexer(inputCol=\"labels5\", outputCol=\"labels5_index\")\n",
        "\n",
        "labels_mapping_pipeline = Pipeline(stages=[Labels2Converter(), Labels5Converter(), labels2_indexer, labels5_indexer])\n",
        "\n",
        "labels2 = ['normal', 'attack']\n",
        "labels5 = ['normal', 'DoS', 'Probe', 'R2L', 'U2R']\n",
        "labels_col = 'labels2_index'"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r6ddqeMfUO9"
      },
      "source": [
        "\n",
        "# Loading train data\n",
        "def loading_train_data():\n",
        "  t0 = time()\n",
        "  train_df = load_dataset(train_nsl_kdd_dataset_path) #----------'#need to add file path'\n",
        "\n",
        "# Fitting preparation pipeline\n",
        "  labels_mapping_model = labels_mapping_pipeline.fit(train_df)\n",
        "\n",
        "# Transforming labels column and adding id column\n",
        "  train_df = labels_mapping_model.transform(train_df).withColumn('id', sql.monotonically_increasing_id())\n",
        "\n",
        "  train_df = train_df.cache()\n",
        "  print(f\"Number of examples in train set: {train_df.count()}\")\n",
        "  print(f\"Time: {time() - t0:.2f}s\")\n",
        "  return loading_train_data\n",
        "#print (loading_train_data()) #Testing purposes"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw5z_VIdfWUF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "e75222f4-332f-43e8-a7bb-3d456b9ad0fd"
      },
      "source": [
        "\n",
        "# Loading test data\n",
        "def loading_test_data():\n",
        "  t0 = time()\n",
        "  test_df = load_dataset(test_nsl_kdd_dataset_path) #----------'#need to add file path'\n",
        "\n",
        "# Transforming labels column and adding id column\n",
        "  test_df = labels_mapping_model.transform(test_df).withColumn('id', sql.monotonically_increasing_id())\n",
        "\n",
        "  test_df = test_df.cache()\n",
        "  print(f\"Number of examples in test set: {test_df.count()}\")\n",
        "  print(f\"Time: {time() - t0:.2f}s\")\n",
        "  return loading_test_data\n",
        "#print (loading_test_data()) #Testing purposes"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e1492bf1ab14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time: {time() - t0:.2f}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloading_test_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloading_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Testing purposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-e1492bf1ab14>\u001b[0m in \u001b[0;36mloading_test_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Loading test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloading_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_nsl_kdd_dataset_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#----------'#need to add file path'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
          ]
        }
      ]
    }
  ]
}